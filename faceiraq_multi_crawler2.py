#!/usr/bin/env python3
"""
faceiraq.org ë©€í‹° ì„¹ì…˜ í¬ë¡¤ëŸ¬
ì •ì¹˜, ì•ˆë³´, ê²½ì œ ì„¹ì…˜ì„ ëª¨ë‘ ìˆ˜ì§‘í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python3 faceiraq_multi_crawler2.py [--hours HOURS] [--sections SECTIONS]
    
ì˜ˆì‹œ:
    # ëª¨ë“  ì„¹ì…˜ ìˆ˜ì§‘ (ê¸°ë³¸)
    python3 faceiraq_multi_crawler2.py
    
    # íŠ¹ì • ì„¹ì…˜ë§Œ ìˆ˜ì§‘
    python3 faceiraq_multi_crawler2.py --sections politics,security
    
    # 48ì‹œê°„ ë²”ìœ„ë¡œ ìˆ˜ì§‘
    python3 faceiraq_multi_crawler2.py --hours 48
    
ì¶œë ¥:
    faceiraq_politics_YYYYMMDD_HHMMSS.json/csv
    faceiraq_security_YYYYMMDD_HHMMSS.json/csv
    faceiraq_economy_YYYYMMDD_HHMMSS.json/csv
"""

import json
import time
import re
import argparse
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import csv

class FaceIraqMultiCrawler:
    # ì„¹ì…˜ ì •ì˜
    SECTIONS = {
        'politics': {
            'name_kr': 'ì •ì¹˜',
            'name_ar': 'Ø³ÙŠØ§Ø³Ø©',
            'url': 'https://www.faceiraq.org/articles/%D8%B3%D9%8A%D8%A7%D8%B3%D8%A9'
        },
        'security': {
            'name_kr': 'ì•ˆë³´',
            'name_ar': 'Ø£Ù…Ù†',
            'url': 'https://www.faceiraq.org/articles/%D8%A3%D9%85%D9%86'
        },
        'economy': {
            'name_kr': 'ê²½ì œ',
            'name_ar': 'Ø§Ù‚ØªØµØ§Ø¯',
            'url': 'https://www.faceiraq.org/articles/%D8%A7%D9%82%D8%AA%D8%B5%D8%A7%D8%AF'
        }
    }
    
    def __init__(self, hours_limit=24, sections=None):
        """
        Args:
            hours_limit: ìˆ˜ì§‘í•  ê¸°ì‚¬ì˜ ì‹œê°„ ì œí•œ (ê¸°ë³¸ 24ì‹œê°„)
            sections: ìˆ˜ì§‘í•  ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ì„¹ì…˜)
        """
        self.hours_limit = hours_limit
        self.cutoff_time = datetime.utcnow() - timedelta(hours=hours_limit)
        
        # ìˆ˜ì§‘í•  ì„¹ì…˜ ê²°ì •
        if sections is None:
            self.target_sections = list(self.SECTIONS.keys())
        else:
            self.target_sections = [s for s in sections if s in self.SECTIONS]
        
        # Chrome ì˜µì…˜ ì„¤ì •
        chrome_options = Options()
        chrome_options.page_load_strategy = "eager"
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--lang=ar')
        chrome_options.add_argument('--window-size=1920,1080')
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # ì„¹ì…˜ë³„ ê²°ê³¼ ì €ì¥
        self.results = {}
        for section in self.target_sections:
            self.results[section] = {
                'articles': [],
                'seen_titles': set()
            }
    
    def parse_time(self, time_str):
        """
        ì‹œê°„ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
        
        ì§€ì› í˜•ì‹:
        1. "Ù…Ù†Ø° X Ø³Ø§Ø¹Ø§Øª" (Xì‹œê°„ ì „)
        2. "Ù…Ù†Ø° X Ø¯Ù‚ÙŠÙ‚Ø©" (Xë¶„ ì „)
        3. "Ù…Ù†Ø° Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©" (1ì‹œê°„ ì „)
        4. "HH:MM DD-MM-YYYY" (ì ˆëŒ€ ì‹œê°„)
        """
        time_str = time_str.strip()
        
        # íŒ¨í„´ 1: "Ù…Ù†Ø° X Ø³Ø§Ø¹Ø§Øª" (Xì‹œê°„ ì „)
        match = re.search(r'Ù…Ù†Ø°\s+(\d+)\s+Ø³Ø§Ø¹Ø§Øª?', time_str)
        if match:
            hours_ago = int(match.group(1))
            return datetime.utcnow() - timedelta(hours=hours_ago)
        
        # íŒ¨í„´ 2: "Ù…Ù†Ø° X Ø¯Ù‚ÙŠÙ‚Ø©" (Xë¶„ ì „)
        match = re.search(r'Ù…Ù†Ø°\s+(\d+)\s+Ø¯Ù‚ÙŠÙ‚Ø©', time_str)
        if match:
            minutes_ago = int(match.group(1))
            return datetime.utcnow() - timedelta(minutes=minutes_ago)
        
        # íŒ¨í„´ 3: "Ù…Ù†Ø° Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©" (1ì‹œê°„ ì „)
        if 'Ù…Ù†Ø° Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©' in time_str or 'Ù…Ù†Ø° Ø³Ø§Ø¹ØªÙŠÙ†' in time_str:
            hours_ago = 1 if 'ÙˆØ§Ø­Ø¯Ø©' in time_str else 2
            return datetime.utcnow() - timedelta(hours=hours_ago)
        
        # íŒ¨í„´ 4: "HH:MM DD-MM-YYYY" (ì ˆëŒ€ ì‹œê°„)
        match = re.search(r'(\d{1,2}):(\d{2})\s+(\d{1,2})-(\d{1,2})-(\d{4})', time_str)
        if match:
            hour, minute, day, month, year = map(int, match.groups())
            # ì´ë¼í¬ ì‹œê°„ (UTC+3)ì„ UTCë¡œ ë³€í™˜
            iraq_time = datetime(year, month, day, hour, minute)
            utc_time = iraq_time - timedelta(hours=3)
            return utc_time
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
        return datetime.utcnow()
    
    def is_within_time_limit(self, publish_date):
        """ê¸°ì‚¬ê°€ ì‹œê°„ ì œí•œ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸"""
        return publish_date >= self.cutoff_time
    
    def crawl_section(self, section_key):
        """íŠ¹ì • ì„¹ì…˜ í¬ë¡¤ë§"""
        section = self.SECTIONS[section_key]
        url = section['url']
        name_kr = section['name_kr']
        
        print(f"\n{'='*60}")
        print(f"ğŸ“° {name_kr} ì„¹ì…˜ í¬ë¡¤ë§ ì‹œì‘")
        print(f"URL: {url}")
        print(f"{'='*60}\n")
        
        self.driver.get(url)
        time.sleep(3)
        
        scroll_count = 0
        max_scrolls = 10
        old_articles_count = 0
        max_old_articles = 5
        
        while scroll_count < max_scrolls:
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # v-card ì°¾ê¸°
            cards = soup.find_all('div', class_='v-card')
            
            for card in cards:
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_elem = card.find('p', class_='article-title')
                    if not title_elem:
                        continue
                    title = title_elem.get_text(strip=True)
                    
                    # ì¤‘ë³µ ì²´í¬
                    if title in self.results[section_key]['seen_titles']:
                        continue
                    
                    # ì‹œê°„ ì¶”ì¶œ
                    time_elem = card.find('div', class_='v-card-subtitle')
                    time_text = time_elem.get_text(strip=True) if time_elem else ''
                    
                    # ì¶œì²˜ ì¶”ì¶œ
                    img_elem = card.find('img')
                    source = img_elem.get('title', 'Unknown') if img_elem else 'Unknown'
                    
                    # URL ì¶”ì¶œ
                    link_elem = card.find('a', href=True)
                    article_url = 'https://www.faceiraq.org' + link_elem['href'] if link_elem else ''
                    
                    # ì‹œê°„ íŒŒì‹±
                    publish_date = self.parse_time(time_text)
                    
                    # ì‹œê°„ ì œí•œ í™•ì¸
                    if not self.is_within_time_limit(publish_date):
                        old_articles_count += 1
                        if old_articles_count >= max_old_articles:
                            print(f"âœ“ 24ì‹œê°„ ì´ì „ ê¸°ì‚¬ {max_old_articles}ê°œ ë°œê²¬, í¬ë¡¤ë§ ì¢…ë£Œ")
                            return
                        continue
                    
                    # ê¸°ì‚¬ ì •ë³´ ì €ì¥
                    article = {
                        'section': name_kr,
                        'section_key': section_key,
                        'title': title,
                        'publishDate': publish_date.isoformat() + 'Z',
                        'timeText': time_text,
                        'source': source,
                        'url': article_url
                    }
                    
                    self.results[section_key]['articles'].append(article)
                    self.results[section_key]['seen_titles'].add(title)
                    
                    print(f"âœ“ [{name_kr}] {title[:50]}... ({source})")
                
                except Exception as e:
                    continue
            
            # ìŠ¤í¬ë¡¤
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            scroll_count += 1
            
            print(f"  ìŠ¤í¬ë¡¤ {scroll_count}/{max_scrolls} (ìˆ˜ì§‘: {len(self.results[section_key]['articles'])}ê°œ)")
        
        print(f"\nâœ“ {name_kr} ì„¹ì…˜ í¬ë¡¤ë§ ì™„ë£Œ: {len(self.results[section_key]['articles'])}ê°œ ê¸°ì‚¬")
    
    def save_results(self, section_key):
        """ì„¹ì…˜ë³„ ê²°ê³¼ë¥¼ JSON ë° CSV íŒŒì¼ë¡œ ì €ì¥"""
        articles = self.results[section_key]['articles']
        section_name = self.SECTIONS[section_key]['name_kr']
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ìµœì‹ ìˆœ ì •ë ¬
        articles.sort(key=lambda x: x.get('publishDate', ''), reverse=True)
        
        # JSON ì €ì¥
        json_filename = f"faceiraq_{section_key}_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        print(f"âœ“ JSON íŒŒì¼ ì €ì¥: {json_filename}")
        
        # CSV ì €ì¥
        csv_filename = f"faceiraq_{section_key}_{timestamp}.csv"
        with open(csv_filename, 'w', encoding='utf-8', newline='') as f:
            if articles:
                fieldnames = ['section', 'arabic_title', 'korean_title', 'publishDate', 'timeText', 'source', 'url']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                for article in articles:
                    writer.writerow({
                        'section': article.get('section', ''),
                        'arabic_title': article.get('title', ''),
                        'korean_title': '',  # GPTë¡œ ë²ˆì—­ í•„ìš”
                        'publishDate': article.get('publishDate', ''),
                        'timeText': article.get('timeText', ''),
                        'source': article.get('source', ''),
                        'url': article.get('url', '')
                    })
        print(f"âœ“ CSV íŒŒì¼ ì €ì¥: {csv_filename}")
        
        return json_filename, csv_filename
    
    def print_summary(self):
        """ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*60}\n")
        
        total_articles = 0
        for section_key in self.target_sections:
            articles = self.results[section_key]['articles']
            section_name = self.SECTIONS[section_key]['name_kr']
            count = len(articles)
            total_articles += count
            
            print(f"ğŸ“° {section_name}: {count}ê°œ")
            
            # ì¶œì²˜ë³„ í†µê³„
            sources = {}
            for article in articles:
                source = article.get('source', 'Unknown')
                sources[source] = sources.get(source, 0) + 1
            
            print(f"   ì¶œì²˜ë³„:")
            for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
                print(f"   - {source}: {count}ê°œ")
            print()
        
        print(f"âœ… ì´ ìˆ˜ì§‘: {total_articles}ê°œ ê¸°ì‚¬")
        print(f"â° ìˆ˜ì§‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“… ìˆ˜ì§‘ ë²”ìœ„: ìµœê·¼ {self.hours_limit}ì‹œê°„\n")
    
    def run(self):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            print(f"\nğŸš€ faceiraq.org ë©€í‹° ì„¹ì…˜ í¬ë¡¤ëŸ¬ ì‹œì‘")
            print(f"ìˆ˜ì§‘ ì„¹ì…˜: {', '.join([self.SECTIONS[s]['name_kr'] for s in self.target_sections])}")
            print(f"ì‹œê°„ ë²”ìœ„: ìµœê·¼ {self.hours_limit}ì‹œê°„\n")
            
            # ê° ì„¹ì…˜ í¬ë¡¤ë§
            for section_key in self.target_sections:
                self.crawl_section(section_key)
            
            # ê²°ê³¼ ì €ì¥
            print(f"\n{'='*60}")
            print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
            print(f"{'='*60}\n")
            
            for section_key in self.target_sections:
                if self.results[section_key]['articles']:
                    self.save_results(section_key)
            
            # ìš”ì•½ ì¶œë ¥
            self.print_summary()
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            traceback.print_exc()
        
        finally:
            self.driver.quit()


def main():
    parser = argparse.ArgumentParser(description='faceiraq.org ë©€í‹° ì„¹ì…˜ í¬ë¡¤ëŸ¬')
    parser.add_argument('--hours', type=int, default=24,
                        help='ìˆ˜ì§‘í•  ì‹œê°„ ë²”ìœ„ (ê¸°ë³¸: 24ì‹œê°„)')
    parser.add_argument('--sections', type=str, default=None,
                        help='ìˆ˜ì§‘í•  ì„¹ì…˜ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: politics,security,economy)')
    
    args = parser.parse_args()
    
    # ì„¹ì…˜ íŒŒì‹±
    sections = None
    if args.sections:
        sections = [s.strip() for s in args.sections.split(',')]
        # ìœ íš¨í•œ ì„¹ì…˜ë§Œ í•„í„°ë§
        valid_sections = list(FaceIraqMultiCrawler.SECTIONS.keys())
        sections = [s for s in sections if s in valid_sections]
        if not sections:
            print(f"âŒ ìœ íš¨í•œ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì„¹ì…˜: {', '.join(valid_sections)}")
            return
    
    crawler = FaceIraqMultiCrawler(hours_limit=args.hours, sections=sections)
    crawler.run()


if __name__ == "__main__":
    main()
